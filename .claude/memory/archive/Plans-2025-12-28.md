# Plans - タスク管理

このファイルは Claude Code Harness のタスク管理ファイルです。
Solo モードで運用中。

---

## 🎯 プロジェクト: 汎用スクレイピングフレームワーク（Web UI版）

### 概要
- **目的**: ブラウザベースで簡単に使える個人用スクレイピングツール
- **対象**: 自分だけ（個人用）
- **参考**: Scrapy のアーキテクチャ
- **スコープ**: 静的HTMLのスクレイピング（MVP）
- **実行環境**: ローカル（localhost）

### 技術スタック
- **UI**: Streamlit
- **言語**: Python 3.10+
- **HTTPクライアント**: requests
- **HTMLパーサー**: BeautifulSoup4 + lxml
- **データ処理**: pandas（テーブル表示用）
- **テスト**: pytest

---

## 🔴 フェーズ1: 基盤構築 `cc:完了`

- [x] プロジェクト初期化 ✅
  - プロジェクトディレクトリ作成: `/Users/donaichu/Workspaces/projects/scraper-framework`
  - pyproject.toml 作成
  - requirements.txt 作成
  - .gitignore 作成
  - README.md 作成
- [x] 基本セットアップ ✅
  - linter（ruff）設定
  - formatter（black）設定
  - pytest 設定
- [x] ディレクトリ構造設計 ✅
  ```
  scraper-framework/
  ├── app.py              # Streamlit エントリーポイント
  ├── scraper/
  │   ├── __init__.py
  │   ├── fetcher.py      # HTTP リクエスト処理
  │   ├── parser.py       # HTML パース
  │   └── extractor.py    # データ抽出
  ├── tests/
  │   └── __init__.py
  ├── requirements.txt
  ├── pyproject.toml
  └── README.md
  ```
- [x] Git 初期化 & 初回コミット ✅
  - Commit: 60e5dc1

---

## 🟡 フェーズ2: コア機能（必須） `cc:完了`

### バックエンドコア

- [x] HTTPリクエスト処理 ✅
  - `fetcher.py`: URL からHTML取得
  - ユーザーエージェント設定
  - タイムアウト設定
  - エラーハンドリング（404, 500, etc）
  - SSRF保護（プライベートIP/メタデータエンドポイントブロック）
  - テスト: `tests/test_fetcher.py` （未作成）

- [x] HTMLパーサー ✅
  - `parser.py`: BeautifulSoup4 でHTML解析
  - CSS セレクター対応
  - XPath セレクター対応（lxml）
  - XPath/CSSインジェクション対策
  - lxmlツリーキャッシング（パフォーマンス最適化）
  - テスト: `tests/test_parser.py` （未作成）

- [x] データ抽出エンジン ✅
  - `extractor.py`: セレクターで要素抽出
  - 単一フィールド抽出
  - 複数フィールド抽出
  - リスト形式でのデータ返却
  - ループ最適化（DRY原則適用）
  - テスト: `tests/test_extractor.py` （未作成）

### Streamlit UI

- [x] 基本UI構築 ✅
  - `app.py`: Streamlit アプリ作成
  - タイトル・説明文（日本語）
  - URL入力フォーム
  - セレクター入力フォーム（CSS/XPath切り替え）
  - 実行ボタン
  - サイドバー設定（User-Agent、タイムアウト）

- [x] 結果表示 ✅
  - Streamlit テーブルで結果表示（pandas DataFrame）
  - エラーメッセージ表示（st.error）詳細な日本語メッセージ
  - 成功メッセージ表示（st.success）
  - Streamlitキャッシング（@st.cache_data）

- [x] 結果ダウンロード機能 ✅
  - JSON ダウンロードボタン
  - CSV ダウンロードボタン
  - pandas → CSV/JSON 変換

### コードレビュー & 修正

- [x] セキュリティレビュー完了 ✅
  - SSRF脆弱性修正（fetcher.py）
  - XPath/CSSインジェクション対策（parser.py）

- [x] パフォーマンスレビュー完了 ✅
  - Streamlitキャッシング導入（app.py）
  - lxmlツリーの遅延初期化（parser.py）

- [x] コード品質レビュー完了 ✅
  - 型ヒント完備（List[Tag], Union types）
  - DRY原則適用（_select_elements ヘルパー）

- [x] アクセシビリティレビュー完了 ✅
  - ラベル・ヘルプテキスト強化
  - エラーメッセージの詳細化

**評価**: C+ → A（全レビュー項目改善完了）
**コミット**: 02d7655

---

## 🟢 フェーズ3: 推奨機能 `cc:TODO`

- [ ] リアルタイムプレビュー `[feature:a11y]`
  - セレクター変更時に即座に結果表示
  - Streamlit のリアクティブ機能活用

- [ ] 複数セレクター対応
  - フィールド名 + セレクター のペア複数入力
  - 動的フィールド追加UI
  - 結果を構造化データで表示

- [ ] レート制限設定 `[feature:a11y]`
  - リクエスト間隔をスライダーで設定
  - time.sleep() で実装

- [ ] ユーザーエージェント設定 `[feature:a11y]`
  - よくあるUAをドロップダウン選択
  - カスタムUA入力欄

- [ ] スクレイピング履歴
  - セッション内で過去の結果を保持（st.session_state）
  - 履歴から再実行
  - 履歴クリアボタン

---

## 🔵 フェーズ4: 仕上げ `cc:TODO`

- [ ] ドキュメント整備
  - README.md に使い方記載
  - 動作環境・インストール手順
  - スクリーンショット追加

- [ ] テスト実行
  - pytest で全テスト実行
  - カバレッジ確認（pytest-cov）

- [ ] セルフレビュー
  - コード品質チェック（ruff/flake8）
  - セキュリティチェック（入力検証） `[feature:security]`
  - エラーハンドリング漏れチェック

- [ ] 動作確認
  - ローカルで Streamlit 起動
  - 実際のWebサイトでスクレイピングテスト
  - エッジケース確認（404, 空レスポンス, etc）

---

## 📦 オプション機能（将来的に） `cc:TODO`

- [ ] 設定の保存/読み込み
  - config.yaml へのエクスポート
  - YAML からのインポート

- [ ] 複数URL一括処理
  - URLリストを貼り付けて一括実行
  - プログレスバー表示

- [ ] ページネーション対応
  - 次ページURLの自動検出
  - 複数ページの結果を統合

- [ ] スケジューリング機能
  - 定期実行（APScheduler）
  - 結果を自動保存

---

## 参考

- [AGENTS.md](./AGENTS.md) - エージェント運用ガイド
- [CLAUDE.md](./CLAUDE.md) - Claude Code 設定

## 技術参考リンク

- [Scrapy Architecture](https://docs.scrapy.org/en/latest/topics/architecture.html)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [BeautifulSoup4 Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
